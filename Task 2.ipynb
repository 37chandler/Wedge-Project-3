{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are not easy to use in their current chronological arrangement, though having them in a large system like GBQ will solve a lot of our problems. Nevertheless, it’ll be convenient to have a local sample of owners to do work. \n",
    "This task asks you to generate a file of owners where the file contains every record for each owner. There will be more than one owner in the file, and I do not want you to include card_no==3, which is the code for non-owners. The size of the sample is up to you, but I’d recommend shooting for a sample that’s around 250 MB. That’s big enough to be rich, but small enough to be fast. Ish.\n",
    "Deliverable\n",
    "A python script that handles the following tasks: \n",
    "1.\tConnects to your GBQ instance.\n",
    "2.\tBuilds a list of owners. \n",
    "3.\tTakes a sample of the owners. \n",
    "4.\tExtracts all records associated with those owners and writes them to a local text file. \n",
    "You’ll submit your code carrying out the steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import sqlite3\n",
    "import db_dtypes\n",
    "\n",
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wedge-project-403222:Transactions\n"
     ]
    }
   ],
   "source": [
    "# These first two values will be different on your machine. \n",
    "service_path = \"\"\n",
    "service_file = 'wedge-project-403222-80aeb3085a6a.json' # change this to your authentication information  \n",
    "\n",
    "gbq_proj_id = 'wedge-project-403222'  \n",
    "\n",
    "# And this should stay the same. \n",
    "private_key = service_path + service_file\n",
    "\n",
    "# Now we pass in our credentials so that Python has permission to access our project.\n",
    "credentials = service_account.Credentials.from_service_account_file(private_key)\n",
    "\n",
    "# And finally we establish our connection\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)\n",
    "\n",
    "for item in client.list_datasets() : \n",
    "    print(item.full_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query=\"\"\"   \n",
    "With rand_cte AS (\n",
    "SELECT Distinct card_no\n",
    "FROM `wedge-project-403222.Transactions.transArchive*`\n",
    "Where card_no != 3\n",
    "ORDER BY RAND()\n",
    "LIMIT 10\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM `wedge-project-403222.Transactions.transArchive*` AS transactions\n",
    "JOIN rand_cte\n",
    "On rand_cte.card_no = transactions.card_no\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 Cannot read field of type STRING as FLOAT64 Field: display\n\nLocation: US\nJob ID: 1d8274d2-2ad1-49c2-b9bf-eeb2a0af20ca\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ethan\\OneDrive\\Documents\\UM\\ADA\\Wedge-Project\\Task 2.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%202.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mquery(Query)\u001b[39m.\u001b[39;49mto_dataframe()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%202.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df\u001b[39m.\u001b[39mto\u001b[39m.\u001b[39mcsv(\u001b[39m'\u001b[39m\u001b[39mowners.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1859\u001b[0m, in \u001b[0;36mQueryJob.to_dataframe\u001b[1;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype)\u001b[0m\n\u001b[0;32m   1693\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_dataframe\u001b[39m(\n\u001b[0;32m   1694\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1695\u001b[0m     bqstorage_client: Optional[\u001b[39m\"\u001b[39m\u001b[39mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1708\u001b[0m     timestamp_dtype: Union[Any, \u001b[39mNone\u001b[39;00m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1709\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpandas.DataFrame\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1710\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \n\u001b[0;32m   1712\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m \u001b[39m            :mod:`shapely` library cannot be imported.\u001b[39;00m\n\u001b[0;32m   1858\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1859\u001b[0m     query_result \u001b[39m=\u001b[39m wait_for_query(\u001b[39mself\u001b[39;49m, progress_bar_type, max_results\u001b[39m=\u001b[39;49mmax_results)\n\u001b[0;32m   1860\u001b[0m     \u001b[39mreturn\u001b[39;00m query_result\u001b[39m.\u001b[39mto_dataframe(\n\u001b[0;32m   1861\u001b[0m         bqstorage_client\u001b[39m=\u001b[39mbqstorage_client,\n\u001b[0;32m   1862\u001b[0m         dtypes\u001b[39m=\u001b[39mdtypes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1873\u001b[0m         timestamp_dtype\u001b[39m=\u001b[39mtimestamp_dtype,\n\u001b[0;32m   1874\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\_tqdm_helpers.py:104\u001b[0m, in \u001b[0;36mwait_for_query\u001b[1;34m(query_job, progress_bar_type, max_results)\u001b[0m\n\u001b[0;32m    100\u001b[0m progress_bar \u001b[39m=\u001b[39m get_progress_bar(\n\u001b[0;32m    101\u001b[0m     progress_bar_type, \u001b[39m\"\u001b[39m\u001b[39mQuery is running\u001b[39m\u001b[39m\"\u001b[39m, default_total, \u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m )\n\u001b[0;32m    103\u001b[0m \u001b[39mif\u001b[39;00m progress_bar \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m query_job\u001b[39m.\u001b[39;49mresult(max_results\u001b[39m=\u001b[39;49mmax_results)\n\u001b[0;32m    106\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    107\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1580\u001b[0m, in \u001b[0;36mQueryJob.result\u001b[1;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[0;32m   1577\u001b[0m     \u001b[39mif\u001b[39;00m retry_do_query \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m job_retry \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1578\u001b[0m         do_get_result \u001b[39m=\u001b[39m job_retry(do_get_result)\n\u001b[1;32m-> 1580\u001b[0m     do_get_result()\n\u001b[0;32m   1582\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mGoogleAPICallError \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m   1583\u001b[0m     exc\u001b[39m.\u001b[39mmessage \u001b[39m=\u001b[39m _EXCEPTION_FOOTER_TEMPLATE\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1584\u001b[0m         message\u001b[39m=\u001b[39mexc\u001b[39m.\u001b[39mmessage, location\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocation, job_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_id\n\u001b[0;32m   1585\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\retry.py:366\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m target \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    363\u001b[0m sleep_generator \u001b[39m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    364\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maximum, multiplier\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiplier\n\u001b[0;32m    365\u001b[0m )\n\u001b[1;32m--> 366\u001b[0m \u001b[39mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    367\u001b[0m     target,\n\u001b[0;32m    368\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predicate,\n\u001b[0;32m    369\u001b[0m     sleep_generator,\n\u001b[0;32m    370\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout,\n\u001b[0;32m    371\u001b[0m     on_error\u001b[39m=\u001b[39;49mon_error,\n\u001b[0;32m    372\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\retry.py:204\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mfor\u001b[39;00m sleep \u001b[39min\u001b[39;00m sleep_generator:\n\u001b[0;32m    203\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39mreturn\u001b[39;00m target()\n\u001b[0;32m    206\u001b[0m     \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    207\u001b[0m     \u001b[39m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1570\u001b[0m, in \u001b[0;36mQueryJob.result.<locals>.do_get_result\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1567\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_do_query \u001b[39m=\u001b[39m retry_do_query\n\u001b[0;32m   1568\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_job_retry \u001b[39m=\u001b[39m job_retry\n\u001b[1;32m-> 1570\u001b[0m \u001b[39msuper\u001b[39;49m(QueryJob, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mresult(retry\u001b[39m=\u001b[39;49mretry, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1572\u001b[0m \u001b[39m# Since the job could already be \"done\" (e.g. got a finished job\u001b[39;00m\n\u001b[0;32m   1573\u001b[0m \u001b[39m# via client.get_job), the superclass call to done() might not\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \u001b[39m# set the self._query_results cache.\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reload_query_results(retry\u001b[39m=\u001b[39mretry, timeout\u001b[39m=\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\job\\base.py:922\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[1;34m(self, retry, timeout)\u001b[0m\n\u001b[0;32m    919\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_begin(retry\u001b[39m=\u001b[39mretry, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    921\u001b[0m kwargs \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m retry \u001b[39mis\u001b[39;00m DEFAULT_RETRY \u001b[39melse\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mretry\u001b[39m\u001b[39m\"\u001b[39m: retry}\n\u001b[1;32m--> 922\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(_AsyncJob, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mresult(timeout\u001b[39m=\u001b[39mtimeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\future\\polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[1;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking_poll(timeout\u001b[39m=\u001b[39mtimeout, retry\u001b[39m=\u001b[39mretry, polling\u001b[39m=\u001b[39mpolling)\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m     \u001b[39m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    263\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "\u001b[1;31mBadRequest\u001b[0m: 400 Cannot read field of type STRING as FLOAT64 Field: display\n\nLocation: US\nJob ID: 1d8274d2-2ad1-49c2-b9bf-eeb2a0af20ca\n"
     ]
    }
   ],
   "source": [
    "df = client.query(Query).to_dataframe()\n",
    "\n",
    "df.to.csv('owners.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
