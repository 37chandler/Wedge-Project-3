{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Big Query is a distributed data warehouse built on a serverless architecture . We’ll discuss this framework in class. In this task you’ll upload all Wedge transaction records to Google Big Query. You’ll want to make sure that the column data types are correctly specified and you’ve properly handled the null values. \n",
    "The requirements for this task change depending on the grade you’re going for. \n",
    "Note: this assignment can be done manually or programmatically. Naturally I’d prefer it be done programmatically so that you get more practice, but that’s not required to get full credit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.api_core.exceptions import NotFound\n",
    "from google.cloud.bigquery import SchemaField\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zip_path = 'Data\\wedge-clean-files.zip'  # Replace with your zip file path\n",
    "extract_path = 'Data'   # Replace with your desired extract path\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export GOOGLE_APPLICATION_CREDENTIALS='wedge-project-403222-85fe5b35980b.json'\n",
    "\n",
    "service_path = \"\"\n",
    "service_file = 'wedge-project-403222-80aeb3085a6a.json' # change this to your authentication information  \n",
    "\n",
    "gbq_proj_id = 'wedge-project-403222'  \n",
    "\n",
    "# And this should stay the same. \n",
    "private_key = service_path + service_file\n",
    "\n",
    "# Now we pass in our credentials so that Python has permission to access our project.\n",
    "credentials = service_account.Credentials.from_service_account_file(private_key)\n",
    "\n",
    "# And finally we establish our connection\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Transactions already exists.\n"
     ]
    }
   ],
   "source": [
    "# Check if the dataset exists\n",
    "dataset_id = 'Transactions'\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "\n",
    "try:\n",
    "    client.get_dataset(dataset_ref)\n",
    "    print(f\"Dataset {dataset_id} already exists.\")\n",
    "except NotFound:\n",
    "    # Create the dataset if it does not exist\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset = client.create_dataset(dataset)\n",
    "    print(f\"Dataset {dataset_id} created.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to parse string \" \" at position 12851",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mlib.pyx:2368\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to parse string \" \"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ethan\\OneDrive\\Documents\\UM\\ADA\\Wedge-Project\\Task 1 upload.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%201%20upload.ipynb#X12sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mif\u001b[39;00m filename\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# or whatever file type your datasets are\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%201%20upload.ipynb#X12sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, filename)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%201%20upload.ipynb#X12sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     dataframes[filename] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(file_path, dtype\u001b[39m=\u001b[39;49mdata_types)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:920\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1065\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1104\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1210\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\arrays\\numeric.py:275\u001b[0m, in \u001b[0;36mNumericArray._from_sequence_of_strings\u001b[1;34m(cls, strings, dtype, copy)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    270\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_from_sequence_of_strings\u001b[39m(\n\u001b[0;32m    271\u001b[0m     \u001b[39mcls\u001b[39m, strings, \u001b[39m*\u001b[39m, dtype: Dtype \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    272\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:\n\u001b[0;32m    273\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumeric\u001b[39;00m \u001b[39mimport\u001b[39;00m to_numeric\n\u001b[1;32m--> 275\u001b[0m     scalars \u001b[39m=\u001b[39m to_numeric(strings, errors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mraise\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype_backend\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnumpy_nullable\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_sequence(scalars, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\tools\\numeric.py:222\u001b[0m, in \u001b[0;36mto_numeric\u001b[1;34m(arg, errors, downcast, dtype_backend)\u001b[0m\n\u001b[0;32m    220\u001b[0m coerce_numeric \u001b[39m=\u001b[39m errors \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    221\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 222\u001b[0m     values, new_mask \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmaybe_convert_numeric(  \u001b[39m# type: ignore[call-overload]  # noqa: E501\u001b[39;49;00m\n\u001b[0;32m    223\u001b[0m         values,\n\u001b[0;32m    224\u001b[0m         \u001b[39mset\u001b[39;49m(),\n\u001b[0;32m    225\u001b[0m         coerce_numeric\u001b[39m=\u001b[39;49mcoerce_numeric,\n\u001b[0;32m    226\u001b[0m         convert_to_masked_nullable\u001b[39m=\u001b[39;49mdtype_backend \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m lib\u001b[39m.\u001b[39;49mno_default\n\u001b[0;32m    227\u001b[0m         \u001b[39mor\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(values_dtype, StringDtype),\n\u001b[0;32m    228\u001b[0m     )\n\u001b[0;32m    229\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mlib.pyx:2410\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to parse string \" \" at position 12851"
     ]
    }
   ],
   "source": [
    "folder_path = 'Data\\clean-files'\n",
    "data_types = {\n",
    "    'datetime': 'str',\n",
    "    'register_no': 'Float64',\n",
    "    'emp_no': 'Float64',\n",
    "    'trans_no': 'Float64',\n",
    "    'upc': 'str',\n",
    "    'description': 'str',\n",
    "    'trans_type': 'str',\n",
    "    'trans_subtype': 'str',\n",
    "    'trans_status': 'str',\n",
    "    'department': 'Float64',\n",
    "    'quantity': 'Float64',\n",
    "    'Scale': 'Float64',\n",
    "    'cost': 'Float64',\n",
    "    'unitPrice': 'Float64',\n",
    "    'total': 'Float64',\n",
    "    'regprice': 'Float64',\n",
    "    'altprice': 'Float64',\n",
    "    'tax': 'Float64',\n",
    "    'taxexempt': 'Float64',\n",
    "    'foodstamp': 'Float64',\n",
    "    'wicable': 'Float64',\n",
    "    'discount': 'Float64',\n",
    "    'memDiscount': 'Float64',\n",
    "    'discountable': 'Float64',\n",
    "    'discounttype': 'Float64',\n",
    "    'voided': 'Float64',\n",
    "    'percentDiscount': 'Float64',\n",
    "    'ItemQtty': 'Float64',\n",
    "    'volDiscType': 'Float64',\n",
    "    'volume': 'Float64',\n",
    "    'VolSpecial': 'Float64',\n",
    "    'mixMatch': 'Float64',\n",
    "    'matched': 'Float64',\n",
    "    'memType': 'Float64',\n",
    "    'staff': 'Float64',\n",
    "    'numflag': 'Float64',\n",
    "    'itemstatus': 'Float64',\n",
    "    'tenderstatus': 'Float64',\n",
    "    'charflag': 'str',\n",
    "    'varflag': 'Float64',\n",
    "    'batchHeaderID': 'Float64',\n",
    "    'local': 'Float64',\n",
    "    'organic': 'Float64',\n",
    "    'display': 'Float64',\n",
    "    'receipt': 'Float64',\n",
    "    'card_no': 'Float64',\n",
    "    'store': 'Float64',\n",
    "    'branch': 'Float64',\n",
    "    'match_id': 'Float64',\n",
    "    'trans_id': 'Float64',\n",
    "\n",
    "}\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):  # or whatever file type your datasets are\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        dataframes[filename] = pd.read_csv(file_path, dtype=data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1 = [\n",
    "    SchemaField(\"datetime\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"register_no\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"emp_no\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"trans_no\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"upc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"description\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"trans_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"trans_subtype\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"trans_status\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"department\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"quantity\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"Scale\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"cost\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"unitPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"total\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"regPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"altPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"tax\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"taxexempt\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"foodstamp\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"wicable\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"discount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"memDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"discountable\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"discounttype\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"voided\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"percentDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"ItemQtty\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"volDiscType\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"volume\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"VolSpecial\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"mixMatch\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"matched\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"memType\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"staff\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"numflag\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"itemstatus\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"tenderstatus\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"charflag\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"varflag\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"batchHeaderID\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"local\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"organic\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"display\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"receipt\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"card_no\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"store\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"branch\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"match_id\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"trans_id\", \"INTEGER\", mode=\"NULLABLE\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded transArchive_201001_201003_clean.csv to wedge-project-403222.Transactions.transArchive_201001_201003_clean\n",
      "Uploaded transArchive_201004_201006_clean.csv to wedge-project-403222.Transactions.transArchive_201004_201006_clean\n",
      "Uploaded transArchive_201007_201009_clean.csv to wedge-project-403222.Transactions.transArchive_201007_201009_clean\n"
     ]
    },
    {
     "ename": "BadRequest",
     "evalue": "400 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/wedge-project-403222/jobs?uploadType=resumable: Provided Schema does not match Table wedge-project-403222:Transactions.transArchive_201010_201012_clean. Field taxexempt has changed type from FLOAT to INTEGER",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidResponse\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:2477\u001b[0m, in \u001b[0;36mClient.load_table_from_file\u001b[1;34m(self, file_obj, destination, rewind, size, num_retries, job_id, job_id_prefix, location, project, job_config, timeout)\u001b[0m\n\u001b[0;32m   2476\u001b[0m \u001b[39mif\u001b[39;00m size \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m _MAX_MULTIPART_SIZE:\n\u001b[1;32m-> 2477\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_resumable_upload(\n\u001b[0;32m   2478\u001b[0m         file_obj, job_resource, num_retries, timeout, project\u001b[39m=\u001b[39;49mproject\n\u001b[0;32m   2479\u001b[0m     )\n\u001b[0;32m   2480\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:2879\u001b[0m, in \u001b[0;36mClient._do_resumable_upload\u001b[1;34m(self, stream, metadata, num_retries, timeout, project)\u001b[0m\n\u001b[0;32m   2852\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Perform a resumable upload.\u001b[39;00m\n\u001b[0;32m   2853\u001b[0m \n\u001b[0;32m   2854\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2877\u001b[0m \u001b[39m    is uploaded.\u001b[39;00m\n\u001b[0;32m   2878\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2879\u001b[0m upload, transport \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initiate_resumable_upload(\n\u001b[0;32m   2880\u001b[0m     stream, metadata, num_retries, timeout, project\u001b[39m=\u001b[39;49mproject\n\u001b[0;32m   2881\u001b[0m )\n\u001b[0;32m   2883\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m upload\u001b[39m.\u001b[39mfinished:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:2952\u001b[0m, in \u001b[0;36mClient._initiate_resumable_upload\u001b[1;34m(self, stream, metadata, num_retries, timeout, project)\u001b[0m\n\u001b[0;32m   2948\u001b[0m     upload\u001b[39m.\u001b[39m_retry_strategy \u001b[39m=\u001b[39m resumable_media\u001b[39m.\u001b[39mRetryStrategy(\n\u001b[0;32m   2949\u001b[0m         max_retries\u001b[39m=\u001b[39mnum_retries\n\u001b[0;32m   2950\u001b[0m     )\n\u001b[1;32m-> 2952\u001b[0m upload\u001b[39m.\u001b[39;49minitiate(\n\u001b[0;32m   2953\u001b[0m     transport,\n\u001b[0;32m   2954\u001b[0m     stream,\n\u001b[0;32m   2955\u001b[0m     metadata,\n\u001b[0;32m   2956\u001b[0m     _GENERIC_CONTENT_TYPE,\n\u001b[0;32m   2957\u001b[0m     stream_final\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   2958\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m   2959\u001b[0m )\n\u001b[0;32m   2961\u001b[0m \u001b[39mreturn\u001b[39;00m upload, transport\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\resumable_media\\requests\\upload.py:420\u001b[0m, in \u001b[0;36mResumableUpload.initiate\u001b[1;34m(self, transport, stream, metadata, content_type, total_bytes, stream_final, timeout)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m _request_helpers\u001b[39m.\u001b[39;49mwait_and_retry(\n\u001b[0;32m    421\u001b[0m     retriable_request, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_status_code, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_strategy\n\u001b[0;32m    422\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\resumable_media\\requests\\_request_helpers.py:155\u001b[0m, in \u001b[0;36mwait_and_retry\u001b[1;34m(func, get_status_code, retry_strategy)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     response \u001b[39m=\u001b[39m func()\n\u001b[0;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m _CONNECTION_ERROR_CLASSES \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\resumable_media\\requests\\upload.py:416\u001b[0m, in \u001b[0;36mResumableUpload.initiate.<locals>.retriable_request\u001b[1;34m()\u001b[0m\n\u001b[0;32m    412\u001b[0m result \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39mrequest(\n\u001b[0;32m    413\u001b[0m     method, url, data\u001b[39m=\u001b[39mpayload, headers\u001b[39m=\u001b[39mheaders, timeout\u001b[39m=\u001b[39mtimeout\n\u001b[0;32m    414\u001b[0m )\n\u001b[1;32m--> 416\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_initiate_response(result)\n\u001b[0;32m    418\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\resumable_media\\_upload.py:518\u001b[0m, in \u001b[0;36mResumableUpload._process_initiate_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Process the response from an HTTP request that initiated upload.\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \n\u001b[0;32m    505\u001b[0m \u001b[39mThis is everything that must be done after a request that doesn't\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[39m.. _sans-I/O: https://sans-io.readthedocs.io/\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 518\u001b[0m _helpers\u001b[39m.\u001b[39;49mrequire_status_code(\n\u001b[0;32m    519\u001b[0m     response,\n\u001b[0;32m    520\u001b[0m     (http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mOK, http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mCREATED),\n\u001b[0;32m    521\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_status_code,\n\u001b[0;32m    522\u001b[0m     callback\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_invalid,\n\u001b[0;32m    523\u001b[0m )\n\u001b[0;32m    524\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resumable_url \u001b[39m=\u001b[39m _helpers\u001b[39m.\u001b[39mheader_required(\n\u001b[0;32m    525\u001b[0m     response, \u001b[39m\"\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_headers\n\u001b[0;32m    526\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\resumable_media\\_helpers.py:108\u001b[0m, in \u001b[0;36mrequire_status_code\u001b[1;34m(response, status_codes, get_status_code, callback)\u001b[0m\n\u001b[0;32m    107\u001b[0m         callback()\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mraise\u001b[39;00m common\u001b[39m.\u001b[39mInvalidResponse(\n\u001b[0;32m    109\u001b[0m         response,\n\u001b[0;32m    110\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRequest failed with status code\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    111\u001b[0m         status_code,\n\u001b[0;32m    112\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected one of\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    113\u001b[0m         \u001b[39m*\u001b[39mstatus_codes\n\u001b[0;32m    114\u001b[0m     )\n\u001b[0;32m    115\u001b[0m \u001b[39mreturn\u001b[39;00m status_code\n",
      "\u001b[1;31mInvalidResponse\u001b[0m: ('Request failed with status code', 400, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.CREATED: 201>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ethan\\OneDrive\\Documents\\UM\\ADA\\Wedge-Project\\Task 1 upload.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%201%20upload.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m table_full_id \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mclient\u001b[39m.\u001b[39mproject\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mdataset_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mtable_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%201%20upload.ipynb#W5sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# If the table does not exist, it will be created. If it exists, data will be appended.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%201%20upload.ipynb#W5sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m job \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mload_table_from_dataframe(dataframe, table_full_id, job_config\u001b[39m=\u001b[39;49mbigquery\u001b[39m.\u001b[39;49mLoadJobConfig(schema\u001b[39m=\u001b[39;49mschema1))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%201%20upload.ipynb#W5sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# Wait for the job to complete\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%201%20upload.ipynb#W5sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m job\u001b[39m.\u001b[39mresult()\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:2717\u001b[0m, in \u001b[0;36mClient.load_table_from_dataframe\u001b[1;34m(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\u001b[0m\n\u001b[0;32m   2715\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(tmppath, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m tmpfile:\n\u001b[0;32m   2716\u001b[0m         file_size \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mgetsize(tmppath)\n\u001b[1;32m-> 2717\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_table_from_file(\n\u001b[0;32m   2718\u001b[0m             tmpfile,\n\u001b[0;32m   2719\u001b[0m             destination,\n\u001b[0;32m   2720\u001b[0m             num_retries\u001b[39m=\u001b[39;49mnum_retries,\n\u001b[0;32m   2721\u001b[0m             rewind\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2722\u001b[0m             size\u001b[39m=\u001b[39;49mfile_size,\n\u001b[0;32m   2723\u001b[0m             job_id\u001b[39m=\u001b[39;49mjob_id,\n\u001b[0;32m   2724\u001b[0m             job_id_prefix\u001b[39m=\u001b[39;49mjob_id_prefix,\n\u001b[0;32m   2725\u001b[0m             location\u001b[39m=\u001b[39;49mlocation,\n\u001b[0;32m   2726\u001b[0m             project\u001b[39m=\u001b[39;49mproject,\n\u001b[0;32m   2727\u001b[0m             job_config\u001b[39m=\u001b[39;49mnew_job_config,\n\u001b[0;32m   2728\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m   2729\u001b[0m         )\n\u001b[0;32m   2731\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   2732\u001b[0m     os\u001b[39m.\u001b[39mremove(tmppath)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:2485\u001b[0m, in \u001b[0;36mClient.load_table_from_file\u001b[1;34m(self, file_obj, destination, rewind, size, num_retries, job_id, job_id_prefix, location, project, job_config, timeout)\u001b[0m\n\u001b[0;32m   2481\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_multipart_upload(\n\u001b[0;32m   2482\u001b[0m             file_obj, job_resource, size, num_retries, timeout, project\u001b[39m=\u001b[39mproject\n\u001b[0;32m   2483\u001b[0m         )\n\u001b[0;32m   2484\u001b[0m \u001b[39mexcept\u001b[39;00m resumable_media\u001b[39m.\u001b[39mInvalidResponse \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m-> 2485\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_http_response(exc\u001b[39m.\u001b[39mresponse)\n\u001b[0;32m   2487\u001b[0m \u001b[39mreturn\u001b[39;00m typing\u001b[39m.\u001b[39mcast(LoadJob, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_from_resource(response\u001b[39m.\u001b[39mjson()))\n",
      "\u001b[1;31mBadRequest\u001b[0m: 400 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/wedge-project-403222/jobs?uploadType=resumable: Provided Schema does not match Table wedge-project-403222:Transactions.transArchive_201010_201012_clean. Field taxexempt has changed type from FLOAT to INTEGER"
     ]
    }
   ],
   "source": [
    "# BigQuery client\n",
    "#client = bigquery.Client()\n",
    "\n",
    "# Path to the directory where files are extracted\n",
    "files_path = 'Data\\clean-files' # Update this to your path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loop through the files and upload each to BigQuery\n",
    "for filename in os.listdir(files_path):\n",
    "    if filename.endswith('.csv'):  # Assuming files are in CSV format\n",
    " \n",
    "        file_path = os.path.join(files_path, filename)\n",
    "        dataframe = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "        float_columns = [\n",
    "            \"quantity\", \"cost\", \"unitPrice\", \"total\", \"regPrice\", \"altPrice\",\n",
    "            \"discount\", \"memDiscount\", \"percentDiscount\", \"ItemQtty\", \"VolSpecial\",\n",
    "            \"memType\", \"batchHeaderID\", \"organic\", \"display\"\n",
    "        ]\n",
    "        integer_columns = [\n",
    "            \"register_no\", \"emp_no\", \"trans_no\", \"department\", \"Scale\", \"tax\", \n",
    "            \"taxexempt\", \"foodstamp\", \"wicable\", \"discountable\", \"discounttype\", \n",
    "            \"voided\", \"volDiscType\", \"volume\", \"mixMatch\", \"matched\", \"staff\", \n",
    "            \"numflag\", \"itemstatus\", \"tenderstatus\", \"varflag\", \"local\", \n",
    "            \"receipt\", \"card_no\", \"store\", \"branch\", \"match_id\", \"trans_id\"\n",
    "        ]\n",
    "\n",
    "        # Process float columns\n",
    "        dataframe[float_columns] = dataframe[float_columns].replace(' ', float('nan'))\n",
    "        dataframe[float_columns] = dataframe[float_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        # Process integer columns\n",
    "        dataframe[integer_columns] = dataframe[integer_columns].replace(' ', -1)\n",
    "        dataframe[integer_columns] = dataframe[integer_columns].fillna(-1).astype('int')\n",
    "\n",
    "        project_id = 'wedge-project-403222'\n",
    "        dataset_id = 'Transactions'\n",
    "        table_id = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Define the full table ID\n",
    "        table_full_id = f\"{client.project}.{dataset_id}.{table_id}\"\n",
    "\n",
    "        # If the table does not exist, it will be created. If it exists, data will be appended.\n",
    "        job = client.load_table_from_dataframe(dataframe, table_full_id, job_config=bigquery.LoadJobConfig(schema=schema1))\n",
    "\n",
    "        # Wait for the job to complete\n",
    "        job.result()\n",
    "        print(f\"Uploaded {filename} to {table_full_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
