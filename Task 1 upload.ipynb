{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Big Query is a distributed data warehouse built on a serverless architecture . We’ll discuss this framework in class. In this task you’ll upload all Wedge transaction records to Google Big Query. You’ll want to make sure that the column data types are correctly specified and you’ve properly handled the null values. \n",
    "The requirements for this task change depending on the grade you’re going for. \n",
    "Note: this assignment can be done manually or programmatically. Naturally I’d prefer it be done programmatically so that you get more practice, but that’s not required to get full credit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.api_core.exceptions import NotFound\n",
    "from google.cloud.bigquery import SchemaField\n",
    "import os\n",
    "import numpy as np \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zip_path = 'Data\\wedge-clean-files.zip'  # Replace with your zip file path\n",
    "extract_path = 'Data\\Clean'   # Replace with your desired extract path\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export GOOGLE_APPLICATION_CREDENTIALS='wedge-project-403222-85fe5b35980b.json'\n",
    "\n",
    "service_path = \"\"\n",
    "service_file = 'wedge-project-403222-80aeb3085a6a.json' # change this to your authentication information  \n",
    "\n",
    "gbq_proj_id = 'wedge-project-403222'  \n",
    "\n",
    "# And this should stay the same. \n",
    "private_key = service_path + service_file\n",
    "\n",
    "# Now we pass in our credentials so that Python has permission to access our project.\n",
    "credentials = service_account.Credentials.from_service_account_file(private_key)\n",
    "\n",
    "# And finally we establish our connection\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Transactions already exists.\n"
     ]
    }
   ],
   "source": [
    "# Check if the dataset exists\n",
    "dataset_id = 'Transactions'\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "\n",
    "try:\n",
    "    client.get_dataset(dataset_ref)\n",
    "    print(f\"Dataset {dataset_id} already exists.\")\n",
    "except NotFound:\n",
    "    # Create the dataset if it does not exist\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset = client.create_dataset(dataset)\n",
    "    print(f\"Dataset {dataset_id} created.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'Data\\Clean\\clean-files'\n",
    "data_types = {\n",
    "    'datetime': 'str',\n",
    "    'register_no': 'Float64',\n",
    "    'emp_no': 'Float64',\n",
    "    'trans_no': 'Float64',\n",
    "    'upc': 'str',\n",
    "    'description': 'str',\n",
    "    'trans_type': 'str',\n",
    "    'trans_subtype': 'str',\n",
    "    'trans_status': 'str',\n",
    "    'department': 'Float64',\n",
    "    'quantity': 'Float64',\n",
    "    'Scale': 'Float64',\n",
    "    'cost': 'Float64',\n",
    "    'unitPrice': 'Float64',\n",
    "    'total': 'Float64',\n",
    "    'regPrice': 'Float64',\n",
    "    'altPrice': 'Float64',\n",
    "    'tax': 'Float64',\n",
    "    'taxexempt': 'Float64',\n",
    "    'foodstamp': 'Float64',\n",
    "    'wicable': 'Float64',\n",
    "    'discount': 'Float64',\n",
    "    'memDiscount': 'Float64',\n",
    "    'discountable': 'Float64',\n",
    "    'discounttype': 'Float64',\n",
    "    'voided': 'Float64',\n",
    "    'percentDiscount': 'Float64',\n",
    "    'ItemQtty': 'Float64',\n",
    "    'volDiscType': 'Float64',\n",
    "    'volume': 'Float64',\n",
    "    'VolSpecial': 'Float64',\n",
    "    'mixMatch': 'Float64',\n",
    "    'matched': 'Float64',\n",
    "    'memType': 'Float64',\n",
    "    'staff': 'Float64',\n",
    "    'numflag': 'Float64',\n",
    "    'itemstatus': 'Float64',\n",
    "    'tenderstatus': 'Float64',\n",
    "    'charflag': 'str',\n",
    "    'varflag': 'Float64',\n",
    "    'batchHeaderID': 'Float64',\n",
    "    'local': 'Float64',\n",
    "    'organic': 'Float64',\n",
    "    'display': 'Float64',\n",
    "    'receipt': 'Float64',\n",
    "    'card_no': 'Float64',\n",
    "    'store': 'Float64',\n",
    "    'branch': 'Float64',\n",
    "    'match_id': 'Float64',\n",
    "    'trans_id': 'Float64',\n",
    "\n",
    "}\n",
    "\n",
    "na_values =[' ']\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'): \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path, dtype=data_types, na_values=na_values)\n",
    "\n",
    "        for column, dtype in data_types.items():\n",
    "            if dtype == 'Float64':\n",
    "                df[column] = pd.to_numeric(df[column], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "string_columns = ['trans_subtype', 'trans_status', 'charflag']\n",
    "for col in string_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "# Convert to numeric (float), using NaN for non-convertible values\n",
    "numeric_columns = ['taxexempt', 'wicable', 'percentDiscount', 'memType', \n",
    "                   'itemstatus', 'tenderstatus', 'local', 'organic', \n",
    "                   'receipt', 'match_id']\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Fill NaN values with a default value if required, e.g., 0.0\n",
    "df.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_20324\\682909863.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  types_df = df.applymap(type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with mixed types:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "types_df = df.applymap(type)\n",
    "unique_types_per_column = types_df.nunique()\n",
    "\n",
    "# Identifying columns with more than one data type\n",
    "mixed_type_columns = unique_types_per_column[unique_types_per_column > 1]\n",
    "print(\"Columns with mixed types:\\n\", mixed_type_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1 = [\n",
    "    SchemaField(\"datetime\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"register_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"emp_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"trans_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"upc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"description\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"trans_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"trans_subtype\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"trans_status\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"department\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"quantity\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"Scale\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"cost\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"unitPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"total\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"regPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"altPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"tax\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"taxexempt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"foodstamp\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"wicable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"discount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"memDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"discountable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"discounttype\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"voided\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"percentDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"ItemQtty\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"volDiscType\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"volume\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"VolSpecial\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"mixMatch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"matched\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"memType\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"staff\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"numflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"itemstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"tenderstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"charflag\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"varflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"batchHeaderID\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"local\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"organic\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"display\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"receipt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"card_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"store\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"branch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"match_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    SchemaField(\"trans_id\", \"FLOAT\", mode=\"NULLABLE\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowTypeError",
     "evalue": "Expected bytes, got a 'float' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ethan\\OneDrive\\Documents\\UM\\ADA\\Wedge-Project\\Task 1 upload.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%201%20upload.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m table_full_id \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mclient\u001b[39m.\u001b[39mproject\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mdataset_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mtable_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%201%20upload.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# If the table does not exist, it will be created. If it exists, data will be appended.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%201%20upload.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m job \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mload_table_from_dataframe(dataframe, table_full_id, job_config\u001b[39m=\u001b[39;49mbigquery\u001b[39m.\u001b[39;49mLoadJobConfig(schema\u001b[39m=\u001b[39;49mschema1))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%201%20upload.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Wait for the job to complete\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Documents/UM/ADA/Wedge-Project/Task%201%20upload.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m job\u001b[39m.\u001b[39mresult()\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:2686\u001b[0m, in \u001b[0;36mClient.load_table_from_dataframe\u001b[1;34m(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[39mif\u001b[39;00m parquet_compression \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msnappy\u001b[39m\u001b[39m\"\u001b[39m:  \u001b[39m# adjust the default value\u001b[39;00m\n\u001b[0;32m   2684\u001b[0m         parquet_compression \u001b[39m=\u001b[39m parquet_compression\u001b[39m.\u001b[39mupper()\n\u001b[1;32m-> 2686\u001b[0m     _pandas_helpers\u001b[39m.\u001b[39;49mdataframe_to_parquet(\n\u001b[0;32m   2687\u001b[0m         dataframe,\n\u001b[0;32m   2688\u001b[0m         new_job_config\u001b[39m.\u001b[39;49mschema,\n\u001b[0;32m   2689\u001b[0m         tmppath,\n\u001b[0;32m   2690\u001b[0m         parquet_compression\u001b[39m=\u001b[39;49mparquet_compression,\n\u001b[0;32m   2691\u001b[0m         parquet_use_compliant_nested_type\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2692\u001b[0m     )\n\u001b[0;32m   2693\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2694\u001b[0m     dataframe\u001b[39m.\u001b[39mto_parquet(\n\u001b[0;32m   2695\u001b[0m         tmppath,\n\u001b[0;32m   2696\u001b[0m         engine\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2702\u001b[0m         ),\n\u001b[0;32m   2703\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py:643\u001b[0m, in \u001b[0;36mdataframe_to_parquet\u001b[1;34m(dataframe, bq_schema, filepath, parquet_compression, parquet_use_compliant_nested_type)\u001b[0m\n\u001b[0;32m    636\u001b[0m kwargs \u001b[39m=\u001b[39m (\n\u001b[0;32m    637\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39muse_compliant_nested_type\u001b[39m\u001b[39m\"\u001b[39m: parquet_use_compliant_nested_type}\n\u001b[0;32m    638\u001b[0m     \u001b[39mif\u001b[39;00m _versions_helpers\u001b[39m.\u001b[39mPYARROW_VERSIONS\u001b[39m.\u001b[39muse_compliant_nested_type\n\u001b[0;32m    639\u001b[0m     \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m    640\u001b[0m )\n\u001b[0;32m    642\u001b[0m bq_schema \u001b[39m=\u001b[39m schema\u001b[39m.\u001b[39m_to_schema_fields(bq_schema)\n\u001b[1;32m--> 643\u001b[0m arrow_table \u001b[39m=\u001b[39m dataframe_to_arrow(dataframe, bq_schema)\n\u001b[0;32m    644\u001b[0m pyarrow\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mwrite_table(\n\u001b[0;32m    645\u001b[0m     arrow_table,\n\u001b[0;32m    646\u001b[0m     filepath,\n\u001b[0;32m    647\u001b[0m     compression\u001b[39m=\u001b[39mparquet_compression,\n\u001b[0;32m    648\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    649\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py:586\u001b[0m, in \u001b[0;36mdataframe_to_arrow\u001b[1;34m(dataframe, bq_schema)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[39mfor\u001b[39;00m bq_field \u001b[39min\u001b[39;00m bq_schema:\n\u001b[0;32m    584\u001b[0m     arrow_names\u001b[39m.\u001b[39mappend(bq_field\u001b[39m.\u001b[39mname)\n\u001b[0;32m    585\u001b[0m     arrow_arrays\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 586\u001b[0m         bq_to_arrow_array(get_column_or_index(dataframe, bq_field\u001b[39m.\u001b[39;49mname), bq_field)\n\u001b[0;32m    587\u001b[0m     )\n\u001b[0;32m    588\u001b[0m     arrow_fields\u001b[39m.\u001b[39mappend(bq_to_arrow_field(bq_field, arrow_arrays[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtype))\n\u001b[0;32m    590\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m((field \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m field \u001b[39min\u001b[39;00m arrow_fields)):\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py:302\u001b[0m, in \u001b[0;36mbq_to_arrow_array\u001b[1;34m(series, bq_field)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[39mif\u001b[39;00m field_type_upper \u001b[39min\u001b[39;00m schema\u001b[39m.\u001b[39m_STRUCT_TYPES:\n\u001b[0;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m pyarrow\u001b[39m.\u001b[39mStructArray\u001b[39m.\u001b[39mfrom_pandas(series, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39marrow_type)\n\u001b[1;32m--> 302\u001b[0m \u001b[39mreturn\u001b[39;00m pyarrow\u001b[39m.\u001b[39;49mArray\u001b[39m.\u001b[39;49mfrom_pandas(series, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49marrow_type)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\array.pxi:1116\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.from_pandas\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\array.pxi:340\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\array.pxi:86\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowTypeError\u001b[0m: Expected bytes, got a 'float' object"
     ]
    }
   ],
   "source": [
    "# BigQuery client\n",
    "#client = bigquery.Client()\n",
    "\n",
    "# Path to the directory where files are extracted\n",
    "files_path = 'Data\\Clean\\clean-files' # Update this to your path\n",
    "\n",
    "# Loop through the files and upload each to BigQuery\n",
    "for filename in os.listdir(files_path):\n",
    "    if filename.endswith('.csv'):  # Assuming files are in CSV format\n",
    " \n",
    "        file_path = os.path.join(files_path, filename)\n",
    "        dataframe = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "\n",
    "        float_columns = ['register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale', 'cost', 'unitPrice', \n",
    "        'total', 'regPrice', 'altPrice', 'tax', 'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', 'discountable', \n",
    "        'discounttype', 'voided', 'percentDiscount', 'ItemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', \n",
    "        'memType', 'staff', 'numflag', 'itemstatus', 'tenderstatus', 'varflag', 'batchHeaderID', 'local', 'organic', 'display', \n",
    "        'receipt', 'card_no', 'store', 'branch', 'match_id', 'trans_id']  # Add other float column names here\n",
    "        for col in float_columns:\n",
    "            dataframe[col] = pd.to_numeric(dataframe[col], errors='coerce')\n",
    "        dataframe.fillna(0.0, inplace=True)\n",
    "\n",
    "        project_id = 'wedge-project-403222'\n",
    "        dataset_id = 'Transactions'\n",
    "        table_id = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Define the full table ID\n",
    "        table_full_id = f\"{client.project}.{dataset_id}.{table_id}\"\n",
    "\n",
    "        # If the table does not exist, it will be created. If it exists, data will be appended.\n",
    "        job = client.load_table_from_dataframe(dataframe, table_full_id, job_config=bigquery.LoadJobConfig(schema=schema1))\n",
    "\n",
    "        # Wait for the job to complete\n",
    "        job.result()\n",
    "        print(f\"Uploaded {filename} to {table_full_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
